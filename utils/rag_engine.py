# utils/rag_engine.py - RAG Engine with Confidence Scoring
"""
RAG Engine v·ªõi t√≠ch h·ª£p Confidence Score.

Logic:
1. Gatekeeper filter: L·ªçc claims d·ª±a tr√™n access control
2. Confidence ranking: ∆Øu ti√™n claims c√≥ confidence cao
3. Context building: ƒê∆∞a confidence v√†o context ƒë·ªÉ AI bi·∫øt ƒë·ªô tin c·∫≠y
4. Response generation: AI c√≥ th·ªÉ caveat th√¥ng tin ch∆∞a verified
"""

import numpy as np
from openai import OpenAI
from sentence_transformers import SentenceTransformer

import sys
import os

# Add project root to path for imports
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from config import (
    get_openai_api_key, LLM_MODEL, MAX_TOKENS_RESPONSE, DEFAULT_TOP_K,
    CONFIDENCE_SCORES, MIN_CONFIDENCE_TRUSTED
)
from utils.gatekeeper import gatekeeper_filter, gatekeeper_filter_with_ranking, get_confidence_summary
from utils.embeddings import search_similar


def build_rag_prompt_with_confidence(
    target_user_id: str, 
    context_items: list, 
    query: str
) -> str:
    """
    X√¢y d·ª±ng prompt cho RAG v·ªõi th√¥ng tin v·ªÅ confidence.
    
    Args:
        target_user_id: ID ng∆∞·ªùi ƒë∆∞·ª£c h·ªèi v·ªÅ
        context_items: List c√°c tuples (content, confidence, status)
        query: C√¢u h·ªèi c·ªßa user
        
    Returns:
        Prompt string
    """
    # Build context v·ªõi confidence markers
    context_lines = []
    for content, confidence, status in context_items:
        if confidence >= 0.9:
            marker = "‚úÖ [VERIFIED - EAS Attested]"
        elif confidence >= 0.5:
            marker = "üìé [Has Evidence]"
        else:
            marker = "üìù [Self-Declared]"
        
        context_lines.append(f"{marker} (Confidence: {confidence:.0%})")
        context_lines.append(f"  {content}")
        context_lines.append("")
    
    context_str = "\n".join(context_lines)
    
    return f"""B·∫°n l√† tr·ª£ l√Ω AI gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.
Ch√∫ √Ω v·ªÅ ƒë·ªô tin c·∫≠y c·ªßa th√¥ng tin:
- ‚úÖ [VERIFIED] = Th√¥ng tin ƒë√£ ƒë∆∞·ª£c x√°c th·ª±c tr√™n blockchain (EAS)
- üìé [Has Evidence] = C√≥ b·∫±ng ch·ª©ng ƒëi k√®m (Github, Link, etc.)
- üìù [Self-Declared] = T·ª± khai b√°o, ch∆∞a x√°c th·ª±c

Th√¥ng tin v·ªÅ '{target_user_id}':
{context_str}

C√¢u h·ªèi: {query}

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin tr√™n
2. N·∫øu th√¥ng tin ch·ªâ l√† Self-Declared, h√£y n√≥i r√µ "Theo khai b√°o c·ªßa ng∆∞·ªùi d√πng..." 
3. N·∫øu th√¥ng tin ƒë√£ Verified, c√≥ th·ªÉ n√≥i "ƒê√£ ƒë∆∞·ª£c x√°c th·ª±c r·∫±ng..."
4. N·∫øu kh√¥ng c√≥ ƒë·ªß th√¥ng tin, h√£y n√≥i r√µ"""


def build_rag_prompt(target_user_id: str, context_str: str, query: str) -> str:
    """
    LEGACY: X√¢y d·ª±ng prompt cho RAG (backward compatibility).
    
    Args:
        target_user_id: ID ng∆∞·ªùi ƒë∆∞·ª£c h·ªèi v·ªÅ
        context_str: Context t·ª´ retrieved documents
        query: C√¢u h·ªèi c·ªßa user
        
    Returns:
        Prompt string
    """
    return f"""B·∫°n l√† tr·ª£ l√Ω AI gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.

Th√¥ng tin v·ªÅ '{target_user_id}':
{context_str}

C√¢u h·ªèi: {query}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin tr√™n. N·∫øu kh√¥ng c√≥ ƒë·ªß th√¥ng tin, h√£y n√≥i r√µ."""


def generate_response(prompt: str) -> str:
    """
    Generate response t·ª´ OpenAI.
    
    Args:
        prompt: Prompt ƒë√£ ƒë∆∞·ª£c build
        
    Returns:
        Response string
    """
    client = OpenAI(api_key=get_openai_api_key())
    
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=MAX_TOKENS_RESPONSE
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"L·ªói khi g·ªçi OpenAI API: {str(e)}"


def simple_rag(
    query: str,
    embedder: SentenceTransformer,
    doc_embeddings: np.ndarray,
    documents: list,
    metadata: list,
    target_user_id: str,
    viewer_id: str,
    viewer_role: str = "Default",
    top_k: int = DEFAULT_TOP_K,
    use_confidence: bool = True
) -> str:
    """
    RAG function v·ªõi Multi-user logic v√† Confidence Scoring.
    
    Quy tr√¨nh:
    1. Filter documents d·ª±a tr√™n gatekeeper logic
    2. T√¨m ki·∫øm documents t∆∞∆°ng t·ª± v·ªõi query
    3. Build context v·ªõi confidence information
    4. Generate response v·ªõi OpenAI (AI bi·∫øt ƒë·ªô tin c·∫≠y)
    
    Args:
        query: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        embedder: SentenceTransformer model
        doc_embeddings: Embeddings c·ªßa t·∫•t c·∫£ documents
        documents: List t·∫•t c·∫£ documents
        metadata: List metadata
        target_user_id: ID ng∆∞·ªùi b·ªã soi
        viewer_id: ID ng∆∞·ªùi ƒëang xem
        viewer_role: Role ƒë·∫∑c bi·ªát
        top_k: S·ªë l∆∞·ª£ng documents retrieve
        use_confidence: C√≥ d√πng confidence scoring kh√¥ng
    
    Returns:
        C√¢u tr·∫£ l·ªùi t·ª´ OpenAI
    """
    # Validate input
    if len(documents) == 0:
        return "Kh√¥ng c√≥ d·ªØ li·ªáu trong h·ªá th·ªëng."
    
    # Step 1: Filter indices d·ª±a tr√™n gatekeeper logic
    allowed_indices = gatekeeper_filter(metadata, target_user_id, viewer_id, viewer_role)
    
    if not allowed_indices:
        return f"Kh√¥ng c√≥ d·ªØ li·ªáu n√†o c·ªßa '{target_user_id}' m√† b·∫°n ƒë∆∞·ª£c ph√©p truy c·∫≠p v·ªõi quy·ªÅn hi·ªán t·∫°i."
    
    # Step 2: Search similar documents
    distances, indices = search_similar(
        embedder=embedder,
        query=query,
        doc_embeddings=doc_embeddings,
        allowed_indices=allowed_indices,
        top_k=top_k
    )
    
    # Step 3: Build context
    if use_confidence:
        # Build context v·ªõi confidence info
        context_items = []
        for i in indices[0]:
            if i != -1:
                actual_idx = allowed_indices[i]
                content = documents[actual_idx]
                confidence = metadata[actual_idx].get("confidence_score", CONFIDENCE_SCORES['base_self_declared'])
                status = metadata[actual_idx].get("status", "self_declared")
                context_items.append((content, confidence, status))
        
        # Generate v·ªõi confidence-aware prompt
        prompt = build_rag_prompt_with_confidence(target_user_id, context_items, query)
    else:
        # Legacy: simple context
        contexts = [documents[allowed_indices[i]] for i in indices[0] if i != -1]
        context_str = "\n".join(contexts)
        prompt = build_rag_prompt(target_user_id, context_str, query)
    
    # Step 4: Generate response
    return generate_response(prompt)


def rag_with_confidence_summary(
    query: str,
    embedder: SentenceTransformer,
    doc_embeddings: np.ndarray,
    documents: list,
    metadata: list,
    target_user_id: str,
    viewer_id: str,
    viewer_role: str = "Default",
    top_k: int = DEFAULT_TOP_K
) -> tuple:
    """
    RAG v·ªõi tr·∫£ v·ªÅ th√™m confidence summary.
    
    Returns:
        Tuple: (response, confidence_summary_dict)
    """
    # Get allowed indices
    allowed_indices = gatekeeper_filter(metadata, target_user_id, viewer_id, viewer_role)
    
    if not allowed_indices:
        return (
            f"Kh√¥ng c√≥ d·ªØ li·ªáu n√†o c·ªßa '{target_user_id}' m√† b·∫°n ƒë∆∞·ª£c ph√©p truy c·∫≠p.",
            {"total": 0, "high_confidence": 0, "medium_confidence": 0, "low_confidence": 0, "avg_confidence": 0.0}
        )
    
    # Get confidence summary
    summary = get_confidence_summary(metadata, allowed_indices)
    
    # Run RAG
    response = simple_rag(
        query=query,
        embedder=embedder,
        doc_embeddings=doc_embeddings,
        documents=documents,
        metadata=metadata,
        target_user_id=target_user_id,
        viewer_id=viewer_id,
        viewer_role=viewer_role,
        top_k=top_k,
        use_confidence=True
    )
    
    return response, summary
